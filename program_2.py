# -*- coding: utf-8 -*-
"""Program 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rxs5_yFJDe4GBJW0nbGb4RyG9iQJrWmH
"""

import gensim.downloader as api
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# Load pre-trained word vectors (Google News Word2Vec)
word_vectors = api.load("word2vec-google-news-300")

# Select 10 words related to 'Technology'
words = ["computer", "laptop", "AI", "machine", "robot", "software", "hardware", "algorithm", "network", "cybersecurity"]

# Get vector representations
vectors = np.array([word_vectors[word] for word in words])

# Function to visualize embeddings
def plot_embeddings(vectors, words, method="PCA"):
    if method == "PCA":
        reduced = PCA(n_components=2).fit_transform(vectors)
    else:
        reduced = TSNE(n_components=2, perplexity=5, random_state=42).fit_transform(vectors)

    plt.figure(figsize=(8, 6))
    plt.scatter(reduced[:, 0], reduced[:, 1])

    for i, word in enumerate(words):
        plt.annotate(word, (reduced[i, 0], reduced[i, 1]), fontsize=12)

    plt.title(f"Word Embedding Visualization using {method}")
    plt.show()

# PCA Visualization
plot_embeddings(vectors, words, method="PCA")

# t-SNE Visualization
plot_embeddings(vectors, words, method="t-SNE")

"""This code demonstrates **word embedding visualization** using **Google News Word2Vec embeddings** and two dimensionality reduction techniques:  
✅ **PCA (Principal Component Analysis)**  
✅ **t-SNE (t-Distributed Stochastic Neighbor Embedding)**  

---

### **How It Works**
1. **Load Pre-Trained Word2Vec Model**  
   - The model (`word2vec-google-news-300`) is a **300-dimensional word vector space** trained on Google News.
   - Words with similar meanings are closer in this space.

2. **Select 10 Technology-Related Words**  
   - The words `"computer", "laptop", "AI", "machine", "robot", "software", "hardware", "algorithm", "network", "cybersecurity"` are chosen.

3. **Extract Word Vectors**  
   - The word embeddings for these words are retrieved from the model.

4. **Reduce Dimensionality for Visualization**  
   - **PCA (Principal Component Analysis)** is used to **reduce 300 dimensions to 2** while preserving variance.
   - **t-SNE (t-Distributed Stochastic Neighbor Embedding)** is used for **nonlinear dimensionality reduction**, better capturing clusters.

5. **Plot the Embeddings**  
   - The words are visualized in 2D space.
   - Each point represents a word, and annotations display the words.

---

### **PCA vs. t-SNE**
| **Method** | **Pros** | **Cons** |
|------------|---------|---------|
| **PCA** | Fast, captures global structure | May not show fine-grained clusters well |
| **t-SNE** | Preserves local relationships, better clustering | Slower, non-deterministic results |

---

### **Key Observations**
- Words like `"computer"`, `"laptop"`, and `"AI"` should cluster together, as they are semantically related.
- PCA may show a **linear spread**, whereas t-SNE will likely form **distinct clusters**.
- `"cybersecurity"` may be slightly distant, as its context differs from `"hardware"` and `"software"`.

---

### **Applications**
✅ **Semantic Word Clustering** – Find similar words in a dataset.  
✅ **NLP Interpretability** – Understand how word embeddings represent meaning.  
✅ **Topic Modeling** – Group related words visually.

---

### **Limitations**
❌ **t-SNE is slow** for large datasets.  
❌ **Word2Vec is static**, so it doesn’t handle context like BERT.

---

"""