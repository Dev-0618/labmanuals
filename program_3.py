# -*- coding: utf-8 -*-
"""Program 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D3vmrlM86gTGHy1jMqFmLOJZ3z7UJIFD
"""

import gensim
from gensim.models import Word2Vec
import nltk
from nltk.tokenize import word_tokenize
import string
import pandas as pd


nltk.download('all')

# Downaload the csv file from this kaggle link
# https://www.kaggle.com/datasets/falgunipatel19/biomedical-text-publication-classification

df = pd.read_csv("alldata_1_for_kaggle 2.csv")

medical_corpus = df['a'].to_list()

# Preprocessing: Tokenization & Lowercasing
def preprocess_text(corpus):
    processed = []
    for sentence in corpus:
        tokens = word_tokenize(sentence.lower())  # Convert to lowercase
        tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation
        processed.append(tokens)
    return processed

tokenized_corpus = preprocess_text(medical_corpus)

# Train a Word2Vec model
model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)

# Save & Load the model
model.save("medical_word2vec.model")
model = Word2Vec.load("medical_word2vec.model")

file_name = "data.txt"

with open(file_name, "r") as data:

  medical_corpus = data.readlines()

medical_corpus

model.wv['diabetes']

# Find similarity between two medical terms
similarity = model.wv.similarity("diabetes", "glucose")
print("Similarity between 'diabetes' and 'glucose':", similarity)

# Find words similar to 'diabetes'
print("Words similar to 'diabetes':", model.wv.most_similar("diabetes"))

"""### **Word2Vec for Medical Text Embeddings: Explanation** 🏥💡  

This code trains a **domain-specific Word2Vec model** using a **small medical corpus** and then tests word relationships using similarity metrics.  

---

## **🚀 How It Works?**

### **1️⃣ Preprocessing the Medical Text**
- **Lowercasing** – Ensures uniformity (e.g., “Diabetes” and “diabetes” are treated the same).  
- **Tokenization** – Splits sentences into words.  
- **Removing Punctuation** – Keeps only alphabetic words (e.g., removes commas and periods).  

---

### **2️⃣ Training a Word2Vec Model**
- **Sentences** → Trained using `Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)`  
- **Key Parameters**:  
  ✅ `vector_size=100` → Each word is represented as a **100-dimensional vector**.  
  ✅ `window=5` → Considers **5 words before and after** for context.  
  ✅ `min_count=1` → Includes all words (even if they appear only once).  
  ✅ `workers=4` → Uses **4 CPU cores** for faster training.  

---

### **3️⃣ Testing the Trained Word Embeddings**
✅ **Find Words Similar to ‘Diabetes’**  
- Uses `model.wv.most_similar("diabetes")`  
- It finds words that **appear in similar contexts** (e.g., `"glucose"`, `"insulin"`, `"blood"`, etc.).  

✅ **Measure Similarity Between Two Words**  
- `model.wv.similarity("diabetes", "glucose")`  
- It computes the **cosine similarity** between the two words' vectors.  
- Higher values (closer to **1.0**) mean the words **appear in similar medical contexts**.  

---

## **🔍 Key Observations**
- `"diabetes"` should be **highly similar** to `"glucose"`, `"insulin"`, etc.  
- `"hypertension"` may not be **as close** to `"glucose"` since they belong to different subfields.  
- The trained embeddings **capture the meaning** based on how words appear in the corpus.  

---

## **🌍 Real-World Applications**
✅ **Medical Chatbots** – Understanding context-based word meanings.  
✅ **Clinical Text Analysis** – Grouping similar medical terms.  
✅ **Disease Prediction** – Identifying relationships between symptoms and conditions.  

---

## **📌 Limitations**
❌ Small corpus → May not generalize well.  
❌ Word2Vec doesn’t understand **polysemy** (same word, different meanings).  
❌ Context-free embeddings → Unlike BERT, it **doesn’t consider surrounding words** dynamically.  

---
"""