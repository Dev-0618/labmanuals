# -*- coding: utf-8 -*-
"""Program 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D3vmrlM86gTGHy1jMqFmLOJZ3z7UJIFD
"""

import gensim
from gensim.models import Word2Vec
import nltk
from nltk.tokenize import word_tokenize
import string
import pandas as pd


nltk.download('all')

# Downaload the csv file from this kaggle link
# https://www.kaggle.com/datasets/falgunipatel19/biomedical-text-publication-classification

df = pd.read_csv("alldata_1_for_kaggle 2.csv")

medical_corpus = df['a'].to_list()

# Preprocessing: Tokenization & Lowercasing
def preprocess_text(corpus):
    processed = []
    for sentence in corpus:
        tokens = word_tokenize(sentence.lower())  # Convert to lowercase
        tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation
        processed.append(tokens)
    return processed

tokenized_corpus = preprocess_text(medical_corpus)

# Train a Word2Vec model
model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)

# Save & Load the model
model.save("medical_word2vec.model")
model = Word2Vec.load("medical_word2vec.model")

file_name = "data.txt"

with open(file_name, "r") as data:

  medical_corpus = data.readlines()

medical_corpus

model.wv['diabetes']

# Find similarity between two medical terms
similarity = model.wv.similarity("diabetes", "glucose")
print("Similarity between 'diabetes' and 'glucose':", similarity)

# Find words similar to 'diabetes'
print("Words similar to 'diabetes':", model.wv.most_similar("diabetes"))

"""### **Word2Vec for Medical Text Embeddings: Explanation** ğŸ¥ğŸ’¡  

This code trains a **domain-specific Word2Vec model** using a **small medical corpus** and then tests word relationships using similarity metrics.  

---

## **ğŸš€ How It Works?**

### **1ï¸âƒ£ Preprocessing the Medical Text**
- **Lowercasing** â€“ Ensures uniformity (e.g., â€œDiabetesâ€ and â€œdiabetesâ€ are treated the same).  
- **Tokenization** â€“ Splits sentences into words.  
- **Removing Punctuation** â€“ Keeps only alphabetic words (e.g., removes commas and periods).  

---

### **2ï¸âƒ£ Training a Word2Vec Model**
- **Sentences** â†’ Trained using `Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)`  
- **Key Parameters**:  
  âœ… `vector_size=100` â†’ Each word is represented as a **100-dimensional vector**.  
  âœ… `window=5` â†’ Considers **5 words before and after** for context.  
  âœ… `min_count=1` â†’ Includes all words (even if they appear only once).  
  âœ… `workers=4` â†’ Uses **4 CPU cores** for faster training.  

---

### **3ï¸âƒ£ Testing the Trained Word Embeddings**
âœ… **Find Words Similar to â€˜Diabetesâ€™**  
- Uses `model.wv.most_similar("diabetes")`  
- It finds words that **appear in similar contexts** (e.g., `"glucose"`, `"insulin"`, `"blood"`, etc.).  

âœ… **Measure Similarity Between Two Words**  
- `model.wv.similarity("diabetes", "glucose")`  
- It computes the **cosine similarity** between the two words' vectors.  
- Higher values (closer to **1.0**) mean the words **appear in similar medical contexts**.  

---

## **ğŸ” Key Observations**
- `"diabetes"` should be **highly similar** to `"glucose"`, `"insulin"`, etc.  
- `"hypertension"` may not be **as close** to `"glucose"` since they belong to different subfields.  
- The trained embeddings **capture the meaning** based on how words appear in the corpus.  

---

## **ğŸŒ Real-World Applications**
âœ… **Medical Chatbots** â€“ Understanding context-based word meanings.  
âœ… **Clinical Text Analysis** â€“ Grouping similar medical terms.  
âœ… **Disease Prediction** â€“ Identifying relationships between symptoms and conditions.  

---

## **ğŸ“Œ Limitations**
âŒ Small corpus â†’ May not generalize well.  
âŒ Word2Vec doesnâ€™t understand **polysemy** (same word, different meanings).  
âŒ Context-free embeddings â†’ Unlike BERT, it **doesnâ€™t consider surrounding words** dynamically.  

---
"""